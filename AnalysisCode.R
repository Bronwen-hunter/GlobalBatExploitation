################################################################################
##                                                                            ##
## "Exploring the potential for online data sources to enhance species threat ##
##  mapping using the case study of global bat exploitation"                  ##
##  HUNTER ET AL. (2024) - Conservation Biology                               ##
##                    *R Code used in analysis*                               ##
##                                                                            ## 
################################################################################

#### PACKAGE DEPENDENCIES ####

set.seed(0)

library(tidyverse)
library(dplyr)
library(tidyr)
library(splitstackshape)
library(MASS)
library(pscl)
library(boot)
library(lubridate)
library(sf)
library(rnaturalearth)
library(rnaturalearthdata)
library(maptools)
library(mgcv)
library(tidymv)
library(pscl)
library(patchwork)
library(ggvenn)
library(DescTools)
library(DHARMa)
library(stringr)
library(scales)
library(clipr)
library(broom)


#### 1) Wrangling of IUCN Red List data ####

# Importing the base world map:
world <- ne_countries(scale = "medium", returnclass = "sf")
world_robinson <- st_transform(world, CRS("+proj=robin"))
world_robinson <- world_robinson[world_robinson$sovereignt != 'Antarctica',] # Removing Antarctica

# Importing IUCN Red List Data (aka the ranges of bat species threatened by hunting)
filepath = "Spatial Data/IUCN_ranges/data_0.shp" # User will need to enter the range shapefile from their local directory
range_polygons <- st_transform(st_read(filepath), CRS("+proj=robin")) # Reading in the range polygons

# For each country in the base map, calculate the number of intersecting polygons
# with a unique species name:
for(i in seq(1,nrow(world_robinson))){
  intersection <- st_intersection(world_robinson[i,], range_polygons)
  print(n_distinct(intersection$SCI_NAME))
  world_robinson$IUCN_count[i] <- n_distinct(intersection$ID_NO)
}

# Creating a new dataframe:
HuntedSpeciesCounts <- world_robinson[c('iso_a2', 'IUCN_count')]

#### 2) Online bat exploitation: Spatial patterns ####

country_counts_all <- read_csv("https://raw.githubusercontent.com/Bronwen-hunter/GlobalBatExploitation/main/Data/country_counts_all.csv")
names(country_counts_all)

# Building the glm with poisson error distribution:
glm1 <- glm(OnlineRecords ~ AcademicStudies +InternetUsers + IUCNCount, family='poisson', data=country_counts_all)
testDispersion(glm1, alternative = "greater") # Testing for overdispersion

# Now, building a negative bionomial glm
glm2 <- glm.nb(OnlineRecords ~ AcademicStudies + InternetUsers + IUCNCount, data=country_counts_all)
testDispersion(glm2, alternative = "greater") # Test indicates overdispersion has been addressed (p>0.05)
drop1(glm2, test='Chisq')
summary(glm2)
plot(simulateResiduals(fittedModel = glm2))

# Testing for the most important variables
caret::varImp(glm2)

# Testing to see whether region is a significant predictor vs internet users:
country_counts_all$region_wb<- as.factor(country_counts_all$region_wb )
glm_internetregion <- glm.nb(OnlineRecords ~ InternetUsers + region_wb, data = country_counts_all)
testDispersion(glm_internetregion , alternative = "greater")
summary(glm_internetregion)
drop1(glm_internetregion, test='Chisq')

plot(simulateResiduals(fittedModel = glm_internetregion))

#### 3) Online bat exploitation: Temporal patterns ####

years_summary <- read.csv('https://raw.githubusercontent.com/Bronwen-hunter/GlobalBatExploitation/main/Data/years_summary.csv')

### Looking at trends through time using a GAM (10 knots):

model_gam <- gam(n_online ~ s(Year, k=10), family = poisson, data=years_summary %>% filter(Year>1990) %>% filter(Year <2023))
gam.check(model_gam)            # Inspecting the model
summary.gam(model_gam)          # Look at the model summary
preds <- predict_gam(model_gam) # Generating predictions frmo the GAM
preds$value <- exp(preds$fit)   # Getting the exponential of the predictions

colors <- c("GAM Prediction" = "#003C78", "Internet Users" = '#0B7519')

# Plotting the model predictions:
ggplot(years_summary %>% filter(Year>1990 ) %>% filter(Year <2023)) +
  geom_bar(aes(x=Year, y=n_online), stat='identity', fill="light grey") +
  geom_line(aes(x=Year, y=Number.of.Internet.users/50000000, color='Internet Users'), stat='identity') +
  geom_line(data=preds, aes(x=Year, y=value, color='GAM Prediction')) +
  scale_colour_manual(values=colors) +
  scale_x_continuous('Year', breaks=seq(1980, 2020, by=5)) +
  scale_y_continuous('Number of Records',
                     labels = ~ format(.x, scientific = FALSE),
                     breaks=seq(0, 175, by=25),
                     sec.axis=sec_axis(~.*50000000,
                                       labels = c(0, '2 Billion', '4 Billion', '6 Billion'),
                                       breaks = c(0, 2000000000,4000000000, 6000000000),
                                       name="Number of internet users")) + 
  theme_bw() +
  theme(legend.title = element_blank(), 
        legend.position=c(0.21, 0.71), 
        legend.box.background = element_rect(colour = "black"),
        legend.text=element_text(size=10),
        axis.title.x=element_text(size=11),
        axis.title.y=element_text(size=11, vjust=1, hjust=0.5),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())

### Looking at trends over time by region:

# Here, the data have been separated by region:
region_year_summary <- read.csv('https://raw.githubusercontent.com/Bronwen-hunter/GlobalBatExploitation/main/Data/region_years_count.csv')


# Constructing the GAM:
model2_gam <- gam(n~ region_un + s(year, by=factor(region_un)), 
                  family=poisson, data=region_year_summary)
summary(model2_gam)

preds_2 <- predict_gam(model2_gam, values = NULL) # Generating predictions
preds_2$se.value <- exp(preds_2$se.fit)
preds_2$value <- exp(preds_2$fit)

preds_2 %>% ggplot() + 
  geom_line(aes(x=year, y=exp(fit))) +
  scale_x_continuous('Year', breaks=seq(1990, 2022, by=4)) +
  scale_y_continuous('Number of Records (predicted)') +
  facet_wrap(~region_un) + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, vjust=0.5), axis.title.x = element_text(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())

years_summary


# Against the background of trends in internet users, is year significant?
years_summary2 <- years_summary %>% filter(Year>1990) %>% filter(Year <2023)
years_summary2$Year <- as.factor(years_summary2$Year)
year_model <- glm(n_online ~  Number.of.Internet.users + Year, family='poisson', data=years_summary2)
summary(year_model)
drop1(year_model, test='Chisq')

# Testing whether number of online sources each year is predicted by the number
# of academic studies covering each year:
glm_temporal_all<- glm.nb(n_online~ academic_n, data=years_summary) # Model with all years 
glm_temporal_filtered<- glm.nb(n_online~ academic_n, data=years_summary %>% filter(Year>1990)) # Models including only years >1990
testDispersion(glm_temporal_filtered)
testDispersion(glm_temporal_all)
plot(simulateResiduals(fittedModel = glm_temporal_filtered))
summary(glm_temporal_filtered)

#### 4) Post-hoc content analysis ####

# This section provides code used to identify most-important terms used in 2014
# and 2020 - We cannot, however, share the raw data, due to privacy concerns.
# Note: files should have a column 'text' contained the main text and 'year'

online_text <- read.csv("" , na.string = 'NA') # Users should input their local file here

# Function for text preprocessing:
preprocess_texts <- function(text){
  text = tolower(text) # Making everything lowercase
  text = gsub("[^\x01-\x7F]", "", text) #Removing emojis
  text = gsub("http.*","", text) # Removing links
  text = gsub("www.*","", text) # Removing links
  text = gsub("\n", " ", text) # Removing line spaces
  text = gsub('[[:punct:]]+','',text) # Removing punctuation
  text = gsub('[0-9.]', '', text) # Removing numbers 
  text = gsub("\\s[A-Za-z](?= )", "", text, perl = TRUE) # Removing single letters
  return(unlist(str_squish(text)))
}

# Loading in stopwords - removing these very common words gives us a better idea
# of words that are unusual or important in each year:
stopWords <- stopwords("en")
mystopwords <- tibble(word = stopWords)

# Preprocessing the texts
online_text$text_preprocessed <- lapply(online_text$text, preprocess_texts)

# Counting the frequency of each word in each year:
online_all_words <- online_text%>%
  tidytext::unnest_tokens(word, text_preprocessed)%>%
  count(year, word, sort = TRUE)

# Removing stopwords from the counts:
online_all_words <- anti_join(online_all_words, mystopwords, 
                              by = "word")

# Calculating TF-IDF (term-frequency, inverse-document frequency) for each word:
online_all_words %>% tidytext::bind_tf_idf(word, year, n)

# Grouping the frequency table by year and including the top 15 words for each year:
plot_words <-  online_all_words %>%
  tidytext::bind_tf_idf(word, year, n) %>%
  group_by(year) %>% 
  slice_max(tf_idf, n = 15) %>%
  ungroup() %>%
  mutate(word = fct_reorder(word, tf_idf))

# Plotting the results:
ggplot(plot_words, aes(tf_idf, word, fill = year)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~year, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)


